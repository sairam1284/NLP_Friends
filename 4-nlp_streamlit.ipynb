{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Create a model to implement in Streamlit\n",
    "\n",
    "1. Preprocessing?\n",
    "1. Build a Corex Model\n",
    "1. Test the Model with example sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from corextopic import corextopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('initial_test_dataframe.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is on my data page\n",
    "df['season_number'] = df['season'].apply(lambda x: int(x[:2]))\n",
    "df['episode_number'] = df['season'].apply(lambda x: int(x[-2:]))\n",
    "df = df[df['season_number']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================================\n",
    "\n",
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Remove words within parenthesis which indicate stage directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parens = lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x)\n",
    "test_df['dial_clean'] = test_df['dialogue'].map(parens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Remove any lines where the word count is = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['sentence_length']> 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Remove the main character names from the dialogue, then append the speaker to the dialogue so we know who said what. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(test_df['speaker'].unique())\n",
    "def remove_name(sentence):\n",
    "    sentence = re.sub('[%s]' % re.escape(string.punctuation), ' ', sentence)\n",
    "    words = sentence.split(' ')\n",
    "    sent_clean = ''\n",
    "    for word in words:\n",
    "        if word not in characters:\n",
    "            sent_clean += word + ' '\n",
    "    return sent_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['dial_clean'] = test_df['dial_clean'].apply(lambda x: remove_name(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be used for the Word Cloud\n",
    "test_df_char = test_df.copy()\n",
    "test_df_char['dial_clean'] = test_df['speaker'] + ' ' + test_df['dial_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Remove some choice stop words - Hold off on this for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ours', 'ourselves', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'himself', \"she's\", 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'their', 'theirs', 'themselves', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_word = stopwords.words('english')\n",
    "accepted_words = ['i','me','my','myself','we','our','you','he','him','his','she','her',\n",
    " 'they','them','what','which','who','whom','but','because','against','when','where','why','how',\n",
    " 'no','nor','not']\n",
    "final_stops = [x for x in stop_word if x not in accepted_words]\n",
    "print(final_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Remove some punctuation, symbols, and lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x) # Removes any non-alpha-numeric thing\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower()) #Lowercases everything and removes punctuation\n",
    "\n",
    "test_df['dial_clean'] = test_df['dial_clean'].map(alphanumeric).map(punc_lower)\n",
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. Output the final DataFrame to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_char.drop(columns=['before_speaker', 'after_speaker', 'season'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speaker</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>writers</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>season_number</th>\n",
       "      <th>episode_number</th>\n",
       "      <th>dial_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>The One Where Rachel Has A Baby</td>\n",
       "      <td>Phoebe</td>\n",
       "      <td>I just can’t decide who she looks more alike,...</td>\n",
       "      <td>Scott Silveri</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>Phoebe  I just can’t decide who she looks more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>The One With the Princess Leia Fantasy</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>You really, really need to get some sleep, ho...</td>\n",
       "      <td>Michael Curtis and Gregory S. Malins</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Rachel  You really  really need to get some sl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    episode speaker  \\\n",
       "496         The One Where Rachel Has A Baby  Phoebe   \n",
       "95   The One With the Princess Leia Fantasy  Rachel   \n",
       "\n",
       "                                              dialogue  \\\n",
       "496   I just can’t decide who she looks more alike,...   \n",
       "95    You really, really need to get some sleep, ho...   \n",
       "\n",
       "                                   writers  sentence_length  season_number  \\\n",
       "496                          Scott Silveri               12              8   \n",
       "95    Michael Curtis and Gregory S. Malins                9              3   \n",
       "\n",
       "     episode_number                                         dial_clean  \n",
       "496              23  Phoebe  I just can’t decide who she looks more...  \n",
       "95                1  Rachel  You really  really need to get some sl...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_char.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_char.to_pickle('streamlit_clean_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================\n",
    "\n",
    "# 2. Character Based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Build the common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(dataframe, column, vectorizer, stops, min_df = 0, max_df = 1.0, n_grams = (1, 1)):\n",
    "    if vectorizer == \"cv\":\n",
    "        vec = CountVectorizer(stop_words=stops, binary=True, min_df = min_df, max_df = max_df, ngram_range=n_grams)\n",
    "    elif vectorizer == \"tfidf\":\n",
    "        vec = TfidfVectorizer(stop_words=stops, binary=True, min_df = min_df, max_df = max_df, ngram_range=n_grams)\n",
    "\n",
    "    \n",
    "    doc_word = vec.fit_transform(dataframe[column])\n",
    "    feature_names = vec.get_feature_names()\n",
    "    id2word = dict((v, k) for k, v in vec.vocabulary_.items())\n",
    "    \n",
    "    return doc_word, feature_names, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Applying the Corex Model to see if I can accurately separate the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_anchors = [char.lower() for char in characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/metis/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['she', 'you'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TC:  52.5659814018552\n",
      "0: \n",
      "joey, hey, man, dude, audition, ya, agent, um, actor, whoa\n",
      "1: \n",
      "phoebe, ooh, frank, cause, ursula, alice, pheebs, okay, mike, sergei\n",
      "2: \n",
      "monica, yeah, pete, sweetie, we, no, wedding, guest, kinda, chip\n",
      "3: \n",
      "chandler, umm, joe, janice, yes, wait, kathy, gym, wow, babe\n",
      "4: \n",
      "ross, uh, carol, ben, emily, marcel, susan, paleontology, hanukkah, sister\n",
      "5: \n",
      "rachel, honey, oh, god, hi, barry, thank, rach, ohh, joshua\n"
     ]
    }
   ],
   "source": [
    "#Here is where I build the model\n",
    "vec1 = CountVectorizer(stop_words=final_stops, binary=True)\n",
    "docword1 = vec1.fit_transform(test_df_char['dial_clean'])\n",
    "featnames1 = vec1.get_feature_names()\n",
    "\n",
    "model_char = corextopic.Corex(n_hidden=6, words=list(np.asarray(featnames1)), seed=1, max_iter=200)\n",
    "model_char.fit(docword1, words=list(np.asarray(featnames1)),anchor_strength=20, \n",
    "               anchors=['joey', 'phoebe', 'monica', 'chandler', 'ross', 'rachel'])\n",
    "\n",
    "topics = model_char.get_topics(n_words=10)\n",
    "print(\"TC: \", model_char.tc)\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: \\n'.format(n) + ', '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28231</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28233</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28234</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28235</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28236 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic0    topic1    topic2    topic3    topic4    topic5\n",
       "0      0.999999  0.000001  0.000001  0.000001  0.000001  0.000001\n",
       "1      0.000001  0.999999  0.000001  0.000001  0.000001  0.000001\n",
       "2      0.000001  0.999999  0.000001  0.000001  0.000001  0.000001\n",
       "3      0.000001  0.000001  0.999999  0.000001  0.000001  0.000001\n",
       "4      0.000001  0.000001  0.000001  0.999999  0.000001  0.000001\n",
       "...         ...       ...       ...       ...       ...       ...\n",
       "28231  0.999999  0.000001  0.000001  0.000001  0.000001  0.000001\n",
       "28232  0.000001  0.999999  0.000001  0.000001  0.000001  0.000001\n",
       "28233  0.999999  0.000001  0.000001  0.000001  0.000001  0.000001\n",
       "28234  0.000001  0.999999  0.000001  0.000001  0.000001  0.000001\n",
       "28235  0.999999  0.000001  0.000001  0.000001  0.000001  0.000001\n",
       "\n",
       "[28236 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_prediction = pd.DataFrame(model_char.predict_proba(docword1)[0], columns=['topic'+str(i) for i in range(6)])\n",
    "char_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test out a few sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Interesting, I would have to say seafood\"\n",
    "s2 = \"How you doin?\"\n",
    "s3 = \"I would say being a doctor has given me a great opportunity to save people's lives while also being a student forever because you keep learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>combine_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>interesting  i would have to say seafood how y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                   combine_sentence\n",
       "0      0  interesting  i would have to say seafood how y..."
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine the sentences and turn into a dataframe\n",
    "pred_char_df = pd.DataFrame([s1 + ' '+ s2 + ' '+ s3], columns=['combine_sentence'])\n",
    "pred_char_df['combine_sentence'] = pred_char_df['combine_sentence'].map(alphanumeric).map(punc_lower)\n",
    "pred_char_df.reset_index(inplace=True)\n",
    "\n",
    "pred_char_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10435 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build the Sparse Matrix from the combined sentence\n",
    "pred_doc = vec1.transform(pred_char_df['combine_sentence'])\n",
    "pred_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.e-06, 1.e-06, 1.e-06, 1.e-06, 1.e-06, 1.e-06]]),\n",
       " array([[2.5773616 , 3.09894896, 3.60197019, 3.85048332, 3.60399848,\n",
       "         4.11179904]]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a prediction\n",
    "model_char.predict_proba(pred_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(model_char.predict_proba(pred_doc)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
